{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part2&Part3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "DcBzidhyFb9V",
        "colab_type": "code",
        "outputId": "2678670e-62f8-4cf3-ba88-b97d6360cd69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from google.colab import drive\n",
        "drive.mount('/content/mydrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/mydrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BHvvcsBihLra",
        "colab_type": "code",
        "outputId": "153e4f37-fb82-4e6a-c211-73365d6cf889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import gensim.models.ldamulticore\n",
        "import os\n",
        "from sklearn import model_selection\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Add\n",
        "from keras.layers import Average\n",
        "from keras.layers import Reshape\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import AveragePooling1D\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras import regularizers\n",
        "from keras.backend import mean\n",
        "from keras.backend import sum"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "QYrbvkkIE_J-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Part 2"
      ]
    },
    {
      "metadata": {
        "id": "MbEn9D-4FA9K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path=\"mydrive/My Drive/labeled_articles/\"\n",
        "#load the files from the drive \n",
        "allFiles=[]\n",
        "#every element of list allFiles contains the same text 3 times from every evaluator\n",
        "for f in os.listdir(path):\n",
        "  new=True\n",
        "  for i in range(len(allFiles)):\n",
        "    if(allFiles[i][0][:-5]==f[:-5]):\n",
        "      new=False\n",
        "      allFiles[i].append(f)\n",
        "  if(new):\n",
        "    allFiles.append([f])\n",
        "X=[]\n",
        "Y=[]\n",
        "sentences=[]\n",
        "for i in range(len(allFiles)):\n",
        "  sentences=[]\n",
        "  for j in range(len(allFiles[i])):\n",
        "    sentences.append([])\n",
        "    filepath = path+allFiles[i][j]\n",
        "    with open(filepath) as fp:  \n",
        "      line = fp.readline()\n",
        "      while line:\n",
        "        sentences[j].append(line)\n",
        "        line = fp.readline()\n",
        "  #sentences contain 3 lists,every element of those lists contain a sentence from different evaluators\n",
        "  for k in range(len(sentences[0])):\n",
        "    #ignore comments\n",
        "    if(sentences[0][k][0]!='#'):\n",
        "      #for each sentence we extract the class from the 3 different evaluators\n",
        "      term1=sentences[0][k][:4]\n",
        "      term2=sentences[1][k][:4]\n",
        "      term3=sentences[2][k][:4]\n",
        "      #if 2 evaluators agree for the class of a sentence we accept it,otherwise we ignore the sentence\n",
        "      if(term1==term2 or term1==term3):\n",
        "        X.append(sentences[0][k][4:])\n",
        "        Y.append(term1)\n",
        "      elif(term2==term3):\n",
        "        X.append(sentences[0][k][4:])\n",
        "        Y.append(term2)\n",
        "trainData=[]\n",
        "\n",
        "def readFiles(path,docs):\n",
        "  count=0\n",
        "  for f in os.listdir(path):\n",
        "    filepath=path+f\n",
        "    with open(filepath) as fp:\n",
        "      count=count+1\n",
        "      docs.append(fp.read())\n",
        "      if(count==100):\n",
        "        break\n",
        "  return docs\n",
        "print(len(X))\n",
        "X=readFiles(\"mydrive/My Drive/unlabeled_articles/arxiv_unlabeled/\",X)\n",
        "X=readFiles(\"mydrive/My Drive/unlabeled_articles/jdm_unlabeled/\",X)\n",
        "X=readFiles(\"mydrive/My Drive/unlabeled_articles/plos_unlabeled/\",X)\n",
        "print(len(X))\n",
        "for i in range(len(X)):\n",
        "  Line=remove_stopwords(X[i])\n",
        "  trainData.append(gensim.utils.simple_preprocess(Line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "61dhhAZAG6lz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "firstModel = gensim.models.Word2Vec(\n",
        "        trainData,\n",
        "        size=100,\n",
        "        window=20,\n",
        "        min_count=2,\n",
        "        workers=10)\n",
        "firstModel.train(trainData, total_examples=len(trainData), epochs=10)\n",
        "\n",
        "print(len(list(firstModel.wv.vocab)))\n",
        "secondModel = gensim.models.Word2Vec(\n",
        "        trainData,\n",
        "        size=200,\n",
        "        window=1,\n",
        "        min_count=2,\n",
        "        workers=10)\n",
        "secondModel.train(trainData, total_examples=len(trainData), epochs=10)\n",
        "\n",
        "print(len(list(secondModel.wv.vocab)))\n",
        "\n",
        "firstModel.wv.save_word2vec_format(\"mydrive/My Drive/model_word2vec.kv\", binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DKqXHUfWIo7p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "listOfWords=firstModel.wv.index2word\n",
        "for i in range(10):\n",
        "  print(listOfWords[i])\n",
        "  print(firstModel.wv.most_similar(positive=listOfWords[i],topn=5))\n",
        "  print(\"\\n\")\n",
        "\n",
        "listOfWords2=secondModel.wv.index2word\n",
        "for i in range(10):\n",
        "  print(listOfWords2[i])\n",
        "  print(secondModel.wv.most_similar(positive=listOfWords2[i],topn=5))\n",
        "  print(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KR4_KCHsqw3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Part 3"
      ]
    },
    {
      "metadata": {
        "id": "Ze2N0S6BssH6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path=\"mydrive/My Drive/labeled_articles/\"\n",
        "#load the files from the drive \n",
        "allFiles=[]\n",
        "#every element of list allFiles contains the same text 3 times from every evaluator\n",
        "for f in os.listdir(path):\n",
        "  new=True\n",
        "  for i in range(len(allFiles)):\n",
        "    if(allFiles[i][0][:-5]==f[:-5]):\n",
        "      new=False\n",
        "      allFiles[i].append(f)\n",
        "  if(new):\n",
        "    allFiles.append([f])\n",
        "X=[]\n",
        "Y=[]\n",
        "sentences=[]\n",
        "for i in range(len(allFiles)):\n",
        "  sentences=[]\n",
        "  for j in range(len(allFiles[i])):\n",
        "    sentences.append([])\n",
        "    filepath = path+allFiles[i][j]\n",
        "    with open(filepath) as fp:  \n",
        "      line = fp.readline()\n",
        "      while line:\n",
        "        sentences[j].append(line)\n",
        "        line = fp.readline()\n",
        "  #sentences contain 3 lists,every element of those lists contain a sentence from different evaluators\n",
        "  for k in range(len(sentences[0])):\n",
        "    #ignore comments\n",
        "    if(sentences[0][k][0]!='#'):\n",
        "      #for each sentence we extract the class from the 3 different evaluators\n",
        "      term1=sentences[0][k][:4]\n",
        "      term2=sentences[1][k][:4]\n",
        "      term3=sentences[2][k][:4]\n",
        "      #if 2 evaluators agree for the class of a sentence we accept it,otherwise we ignore the sentence\n",
        "      if(term1==term2 or term1==term3):\n",
        "        X.append(sentences[0][k][4:])\n",
        "        Y.append(term1)\n",
        "      elif(term2==term3):\n",
        "        X.append(sentences[0][k][4:])\n",
        "        Y.append(term2)\n",
        "print(X[0])\n",
        "X_preprossed=[]\n",
        "for sen in X:\n",
        "  sen=gensim.utils.simple_preprocess(remove_stopwords(sen))\n",
        "  fullSen=''\n",
        "  for i in range(len(sen)):\n",
        "    fullSen=fullSen+' ' + sen[i]\n",
        "  X_preprossed.append(fullSen)\n",
        "  \n",
        "print(X_preprossed[:5])\n",
        "  \n",
        "x_train, x_test, y_train, y_test = model_selection.train_test_split(X_preprossed, Y, random_state = 22,stratify=Y)\n",
        "\n",
        "dictLabel={'AIMX':0,'OWNX':1,'CONT':4,'BASE':3,'MISC':2}\n",
        "\n",
        "Ytrain=[dictLabel[label] for label in y_train]\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizerTrain = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizerTrain.fit_on_texts(x_train)\n",
        "\n",
        "max_length = max([len(s.split()) for s in x_train])\n",
        "\n",
        "encoded_docsTrain = tokenizerTrain.texts_to_sequences(x_train)\n",
        "# pad sequences\n",
        "Xtrain = pad_sequences(encoded_docsTrain, maxlen=max_length, padding='post')\n",
        "#print(Xtrain)\n",
        "vocab_size = len(tokenizerTrain.word_index) + 1\n",
        "\n",
        "Ytest=[dictLabel[label] for label in y_test]\n",
        "\n",
        "\n",
        "# create the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(x_test)\n",
        "\n",
        "encoded_docs = tokenizer.texts_to_sequences(x_test)\n",
        "# pad sequences\n",
        "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
        "#print(Xtrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7U4BZUbWwoup",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(vocab_size,max_length)\n",
        "print(Xtrain.shape)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Lambda(lambda x: mean(x, axis=1)))\n",
        "model.add(Dense(16,activation='relu'))\n",
        "#model.add(Flatten())\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, Ytrain, epochs=10, verbose=2)\n",
        "predicts=model.predict(Xtest)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, Ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n",
        "model.predict(Xtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yMVVEELfdKv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Lambda(lambda x: mean(x, axis=1)))\n",
        "#model.add(Flatten())\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, Ytrain, epochs=10, verbose=2)\n",
        "predicts=model.predict(Xtest)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, Ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))\n",
        "for word in predicts:\n",
        "  if word[1]>word[4]:\n",
        "    print (word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RvgvQG78gbj9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Lambda(lambda x: mean(x, axis=1)))\n",
        "#model.add(Flatten())\n",
        "model.add(Dense(5, activation='linear'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, Ytrain, epochs=10, verbose=2)\n",
        "predicts=model.predict(Xtest)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, Ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SvqATud3izu0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(MaxPooling1D(pool_size=2,data_format='channels_last'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Lambda(lambda x: mean(x, axis=1)))\n",
        "#model.add(Flatten())\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, Ytrain, epochs=5, verbose=2)\n",
        "predicts=model.predict(Xtest)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, Ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YRXoGbF-s1GD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Lambda(lambda x: mean(x, axis=1)))\n",
        "#model.add(Flatten())\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "print(model.summary())\n",
        "# compile network\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, Ytrain, epochs=5, verbose=2)\n",
        "predicts=model.predict(Xtest)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, Ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DarHzjlQ5H8z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(max_length)\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(firstModel.wv.get_keras_embedding(train_embeddings=False))\n",
        "model.add(Lambda(lambda x: mean(x, axis=1)))\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "print(\"\\nModel Summary:\\n\", model.summary())\n",
        "\n",
        "           \n",
        "# compile network\n",
        "sgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit network\n",
        "model.fit(Xtrain, Ytrain, epochs=10, verbose=2)\n",
        "# evaluate\n",
        "loss, acc = model.evaluate(Xtest, Ytest, verbose=0)\n",
        "print('Test Accuracy: %f' % (acc*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lYvddiO1pn8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Xtrain"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}